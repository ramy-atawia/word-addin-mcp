#!/usr/bin/env python3
"""
Debug the raw LLM response to see what's actually being returned.
"""

import asyncio
import sys
import os

# Add the backend to the path
sys.path.insert(0, '/Users/Mariam/word-addin-mcp/backend')

from app.services.llm_client import LLMClient
from app.core.config import settings

async def debug_raw_llm():
    """Debug the raw LLM response."""
    print("ğŸ” Debugging Raw LLM Response...")
    
    try:
        # Create LLM client directly
        llm_client = LLMClient(
            azure_openai_api_key=settings.azure_openai_api_key,
            azure_openai_endpoint=settings.azure_openai_endpoint,
            azure_openai_deployment=settings.azure_openai_deployment
        )
        
        print("âœ… LLM client created")
        print(f"ğŸ“Š Model: {llm_client.azure_openai_deployment}")
        print(f"ğŸ“Š Endpoint: {llm_client.azure_openai_endpoint}")
        print(f"ğŸ“Š Available: {llm_client.llm_available}")
        
        if not llm_client.llm_available:
            print("âŒ LLM not available")
            return
        
        # Test with a very simple prompt
        print("ğŸ” Testing simple prompt...")
        simple_prompt = "Hello, how are you?"
        
        # Make the API call directly to see raw response
        messages = [{"role": "user", "content": simple_prompt}]
        
        print("ğŸ” Making direct API call...")
        response = llm_client.client.chat.completions.create(
            model=llm_client.azure_openai_deployment,
            messages=messages,
            max_completion_tokens=100
        )
        
        print(f"âœ… Raw response received")
        print(f"ğŸ“Š Response type: {type(response)}")
        print(f"ğŸ“Š Response attributes: {dir(response)}")
        
        if hasattr(response, 'choices'):
            print(f"ğŸ“Š Choices count: {len(response.choices)}")
            if response.choices:
                choice = response.choices[0]
                print(f"ğŸ“Š Choice type: {type(choice)}")
                print(f"ğŸ“Š Choice attributes: {dir(choice)}")
                
                if hasattr(choice, 'message'):
                    message = choice.message
                    print(f"ğŸ“Š Message type: {type(message)}")
                    print(f"ğŸ“Š Message attributes: {dir(message)}")
                    print(f"ğŸ“Š Message content: {repr(message.content)}")
                    print(f"ğŸ“Š Message content type: {type(message.content)}")
                    print(f"ğŸ“Š Message role: {message.role}")
                    
                    if hasattr(message, 'finish_reason'):
                        print(f"ğŸ“Š Finish reason: {message.finish_reason}")
        
        if hasattr(response, 'usage'):
            usage = response.usage
            print(f"ğŸ“Š Usage: {usage}")
            if usage:
                print(f"ğŸ“Š Prompt tokens: {usage.prompt_tokens}")
                print(f"ğŸ“Š Completion tokens: {usage.completion_tokens}")
                print(f"ğŸ“Š Total tokens: {usage.total_tokens}")
        
        # Test the generate_text method
        print("\nğŸ” Testing generate_text method...")
        result = llm_client.generate_text(simple_prompt, max_tokens=100)
        print(f"ğŸ“Š Result: {result}")
        
    except Exception as e:
        print(f"âŒ Error during debug: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(debug_raw_llm())
