#!/usr/bin/env python3
"""
Debug the LLM response in report generation.
"""

import asyncio
import sys
import os
import json

# Add the backend to the path
sys.path.insert(0, '/Users/Mariam/word-addin-mcp/backend')

from app.services.patent_search_service import PatentSearchService
from app.utils.prompt_loader import load_prompt_template

async def debug_llm_response():
    """Debug the LLM response step by step."""
    print("ğŸ” Debugging LLM Response...")
    
    try:
        # Create the service instance
        service = PatentSearchService()
        print("âœ… Service created")
        
        # Test loading prompts
        print("ğŸ” Testing prompt loading...")
        try:
            system_prompt = load_prompt_template("prior_art_search_system")
            print(f"âœ… System prompt loaded: {len(system_prompt)} chars")
            print(f"ğŸ“„ System prompt preview: {system_prompt[:200]}...")
        except Exception as e:
            print(f"âŒ System prompt loading failed: {e}")
            return
        
        try:
            user_prompt = load_prompt_template("prior_art_search_simple",
                                              user_query="5g ai",
                                              conversation_context="Test context",
                                              document_reference="Test patents")
            print(f"âœ… User prompt loaded: {len(user_prompt)} chars")
            print(f"ğŸ“„ User prompt preview: {user_prompt[:300]}...")
        except Exception as e:
            print(f"âŒ User prompt loading failed: {e}")
            return
        
        # Test LLM call directly
        print("ğŸ” Testing LLM call...")
        try:
            response = service.llm_client.generate_text(
                prompt=user_prompt,
                system_message=system_prompt,
                max_tokens=6000
            )
            
            print(f"âœ… LLM response received")
            print(f"ğŸ“Š Response type: {type(response)}")
            print(f"ğŸ“Š Response keys: {response.keys() if isinstance(response, dict) else 'N/A'}")
            
            if isinstance(response, dict):
                success = response.get("success", False)
                print(f"ğŸ“Š Success: {success}")
                
                if success:
                    text = response.get("text", "")
                    print(f"ğŸ“ Text length: {len(text)}")
                    if text:
                        print(f"ğŸ“„ Text preview: {text[:300]}...")
                    else:
                        print("âŒ Empty text in response")
                else:
                    error = response.get("error", "Unknown error")
                    print(f"âŒ LLM error: {error}")
            else:
                print(f"âŒ Unexpected response type: {type(response)}")
                print(f"Response: {response}")
                
        except Exception as e:
            print(f"âŒ LLM call failed: {e}")
            import traceback
            traceback.print_exc()
            
    except Exception as e:
        print(f"âŒ Error during debug: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(debug_llm_response())
