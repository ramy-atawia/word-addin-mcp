name: Monitoring and Alerting

on:
  schedule:
    # Run every 5 minutes
    - cron: '*/5 * * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to monitor'
        required: true
        default: 'production'
        type: choice
        options:
        - staging
        - production

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  # Health checks
  health-check:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [staging, production]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install monitoring tools
      run: |
        pip install requests psutil

    - name: Health check script
      run: |
        python << 'EOF'
        import requests
        import json
        import time
        from datetime import datetime
        
        # Configuration
        environments = {
            'staging': {
                'backend_url': 'https://staging-api.example.com',
                'frontend_url': 'https://staging.example.com',
                'timeout': 30
            },
            'production': {
                'backend_url': 'https://api.example.com',
                'frontend_url': 'https://example.com',
                'timeout': 30
            }
        }
        
        env = '${{ matrix.environment }}'
        config = environments[env]
        
        results = {
            'environment': env,
            'timestamp': datetime.utcnow().isoformat(),
            'checks': {}
        }
        
        # Backend health check
        try:
            start_time = time.time()
            response = requests.get(
                f"{config['backend_url']}/health",
                timeout=config['timeout']
            )
            response_time = (time.time() - start_time) * 1000
            
            results['checks']['backend'] = {
                'status': 'healthy' if response.status_code == 200 else 'unhealthy',
                'status_code': response.status_code,
                'response_time_ms': round(response_time, 2),
                'timestamp': datetime.utcnow().isoformat()
            }
        except Exception as e:
            results['checks']['backend'] = {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
        
        # Frontend health check
        try:
            start_time = time.time()
            response = requests.get(
                config['frontend_url'],
                timeout=config['timeout']
            )
            response_time = (time.time() - start_time) * 1000
            
            results['checks']['frontend'] = {
                'status': 'healthy' if response.status_code == 200 else 'unhealthy',
                'status_code': response.status_code,
                'response_time_ms': round(response_time, 2),
                'timestamp': datetime.utcnow().isoformat()
            }
        except Exception as e:
            results['checks']['frontend'] = {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
        
        # Write results to file
        with open(f'health-check-{env}.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"Health check completed for {env}")
        print(json.dumps(results, indent=2))
        EOF

    - name: Upload health check results
      uses: actions/upload-artifact@v3
      with:
        name: health-check-${{ matrix.environment }}
        path: health-check-${{ matrix.environment }}.json

  # Performance monitoring
  performance-monitor:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install performance tools
      run: |
        pip install locust psutil

    - name: Performance test script
      run: |
        python << 'EOF'
        import requests
        import time
        import json
        from datetime import datetime
        
        # Performance test configuration
        test_urls = [
            'https://api.example.com/health',
            'https://api.example.com/api/v1/mcp/tools',
            'https://api.example.com/api/v1/chat',
        ]
        
        results = {
            'timestamp': datetime.utcnow().isoformat(),
            'tests': []
        }
        
        for url in test_urls:
            print(f"Testing: {url}")
            
            # Run multiple requests to get average performance
            response_times = []
            status_codes = []
            
            for i in range(10):
                try:
                    start_time = time.time()
                    response = requests.get(url, timeout=30)
                    response_time = (time.time() - start_time) * 1000
                    
                    response_times.append(response_time)
                    status_codes.append(response.status_code)
                    
                    time.sleep(0.1)  # Small delay between requests
                    
                except Exception as e:
                    print(f"Error testing {url}: {e}")
            
            if response_times:
                avg_response_time = sum(response_times) / len(response_times)
                min_response_time = min(response_times)
                max_response_time = max(response_times)
                
                results['tests'].append({
                    'url': url,
                    'average_response_time_ms': round(avg_response_time, 2),
                    'min_response_time_ms': round(min_response_time, 2),
                    'max_response_time_ms': round(max_response_time, 2),
                    'success_rate': len([s for s in status_codes if s == 200]) / len(status_codes) * 100,
                    'total_requests': len(response_times)
                })
        
        # Write results to file
        with open('performance-test-results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("Performance test completed")
        print(json.dumps(results, indent=2))
        EOF

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: performance-test-results.json

  # Error rate monitoring
  error-monitor:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Error monitoring script
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        
        # Error monitoring configuration
        api_endpoints = [
            '/health',
            '/api/v1/mcp/tools',
            '/api/v1/chat',
            '/api/v1/session',
        ]
        
        base_url = 'https://api.example.com'
        results = {
            'timestamp': datetime.utcnow().isoformat(),
            'error_summary': {},
            'endpoint_errors': {}
        }
        
        total_requests = 0
        total_errors = 0
        
        for endpoint in api_endpoints:
            url = base_url + endpoint
            print(f"Monitoring: {url}")
            
            try:
                response = requests.get(url, timeout=30)
                total_requests += 1
                
                if response.status_code >= 400:
                    total_errors += 1
                    error_type = f"{response.status_code // 100}xx"
                    
                    if endpoint not in results['endpoint_errors']:
                        results['endpoint_errors'][endpoint] = {}
                    
                    if error_type not in results['endpoint_errors'][endpoint]:
                        results['endpoint_errors'][endpoint][error_type] = 0
                    
                    results['endpoint_errors'][endpoint][error_type] += 1
                    
            except Exception as e:
                total_errors += 1
                print(f"Error monitoring {endpoint}: {e}")
        
        # Calculate error rates
        error_rate = (total_errors / total_requests * 100) if total_requests > 0 else 0
        
        results['error_summary'] = {
            'total_requests': total_requests,
            'total_errors': total_errors,
            'error_rate_percent': round(error_rate, 2),
            'success_rate_percent': round(100 - error_rate, 2)
        }
        
        # Write results to file
        with open('error-monitoring-results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print("Error monitoring completed")
        print(json.dumps(results, indent=2))
        
        # Alert if error rate is too high
        if error_rate > 5:  # Alert if error rate > 5%
            print("🚨 HIGH ERROR RATE DETECTED!")
            exit(1)
        EOF

    - name: Upload error monitoring results
      uses: actions/upload-artifact@v3
      with:
        name: error-monitoring-results
        path: error-monitoring-results.json

  # Alert on issues
  alert:
    runs-on: ubuntu-latest
    needs: [health-check, performance-monitor, error-monitor]
    if: always()

    steps:
    - name: Check for critical issues
      run: |
        # Check if any health checks failed
        if [ -f "health-check-staging.json" ]; then
          echo "Checking staging health..."
          python -c "
        import json
        with open('health-check-staging.json') as f:
            data = json.load(f)
            for service, check in data['checks'].items():
                if check['status'] != 'healthy':
                    print(f'🚨 {service} is unhealthy in staging')
                    exit(1)
        "
        fi
        
        if [ -f "health-check-production.json" ]; then
          echo "Checking production health..."
          python -c "
        import json
        with open('health-check-production.json') as f:
            data = json.load(f)
            for service, check in data['checks'].items():
                if check['status'] != 'healthy':
                    print(f'🚨 {service} is unhealthy in production')
                    exit(1)
        "
        fi
        
        # Check error rates
        if [ -f "error-monitoring-results.json" ]; then
          echo "Checking error rates..."
          python -c "
        import json
        with open('error-monitoring-results.json') as f:
            data = json.load(f)
            error_rate = data['error_summary']['error_rate_percent']
            if error_rate > 5:
                print(f'🚨 High error rate detected: {error_rate}%')
                exit(1)
        "
        fi

    - name: Send alert on failure
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: '🚨 Critical system issues detected! Check monitoring results immediately.'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: Send success notification
      if: success()
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: '✅ All systems operational - monitoring completed successfully'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  # Generate monitoring report
  report:
    runs-on: ubuntu-latest
    needs: [health-check, performance-monitor, error-monitor]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts

    - name: Generate monitoring report
      run: |
        echo "# System Monitoring Report" > monitoring-report.md
        echo "Generated on: $(date)" >> monitoring-report.md
        echo "" >> monitoring-report.md
        
        # Health check summary
        echo "## Health Check Summary" >> monitoring-report.md
        echo "" >> monitoring-report.md
        
        for env in staging production; do
          if [ -f "artifacts/health-check-$env/health-check-$env.json" ]; then
            echo "### $env Environment" >> monitoring-report.md
            echo "" >> monitoring-report.md
            
            python -c "
        import json
        with open('artifacts/health-check-$env/health-check-$env.json') as f:
            data = json.load(f)
            for service, check in data['checks'].items():
                status_emoji = '✅' if check['status'] == 'healthy' else '❌'
                print(f'- {status_emoji} {service}: {check[\"status\"]}')
                if 'response_time_ms' in check:
                    print(f'  - Response time: {check[\"response_time_ms\"]}ms')
        " >> monitoring-report.md
            
            echo "" >> monitoring-report.md
          fi
        done
        
        # Performance summary
        if [ -f "artifacts/performance-results/performance-test-results.json" ]; then
          echo "## Performance Summary" >> monitoring-report.md
          echo "" >> monitoring-report.md
          
          python -c "
        import json
        with open('artifacts/performance-results/performance-test-results.json') as f:
            data = json.load(f)
            for test in data['tests']:
                print(f'- {test[\"url\"]}')
                print(f'  - Avg response time: {test[\"average_response_time_ms\"]}ms')
                print(f'  - Success rate: {test[\"success_rate\"]:.1f}%')
        " >> monitoring-report.md
          
          echo "" >> monitoring-report.md
        fi
        
        # Error summary
        if [ -f "artifacts/error-monitoring-results/error-monitoring-results.json" ]; then
          echo "## Error Summary" >> monitoring-report.md
          echo "" >> monitoring-report.md
          
          python -c "
        import json
        with open('artifacts/error-monitoring-results/error-monitoring-results.json') as f:
            data = json.load(f)
            summary = data['error_summary']
            print(f'- Total requests: {summary[\"total_requests\"]}')
            print(f'- Total errors: {summary[\"total_errors\"]}')
            print(f'- Error rate: {summary[\"error_rate_percent\"]}%')
            print(f'- Success rate: {summary[\"success_rate_percent\"]}%')
        " >> monitoring-report.md
        fi

    - name: Upload monitoring report
      uses: actions/upload-artifact@v3
      with:
        name: monitoring-report
        path: monitoring-report.md
